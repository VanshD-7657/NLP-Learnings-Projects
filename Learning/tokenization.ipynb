{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65951631",
   "metadata": {},
   "source": [
    "#### Installing the NLTK Library by \n",
    "``` !pip install nltk ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7d0bf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\AMAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\AMAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\AMAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\AMAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\AMAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\AMAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\AMAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# download both punkt and punkt_tab (needed in latest versions)\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "# Download the required POS tagger model if not already present\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5196e07-1a0e-4337-b1b2-1499f2c1e6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\" Hello Welcome, to Vansh NLP Tutorials.\n",
    "Please do watch the entire couse! to become expert in NLP \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e31654a8-4a51-4068-8b2f-3b6994691f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "documents = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daef42a5-695e-4e90-8e45-5aa6f582948f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfdbbe56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  Hello Welcome, to Vansh NLP Tutorials.\n",
      "2 Please do watch the entire couse!\n",
      "3 to become expert in NLP\n"
     ]
    }
   ],
   "source": [
    "for idx,sent in enumerate(documents,start=1):\n",
    "    print(idx,sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fd757d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Welcome', ',', 'to', 'Vansh', 'NLP', 'Tutorials', '.', 'Please', 'do', 'watch', 'the', 'entire', 'couse', '!', 'to', 'become', 'expert', 'in', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "## Paragraph--> words\n",
    "## Sentence--> words\n",
    "from nltk import word_tokenize\n",
    "word = word_tokenize(corpus)\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "597dc413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. words are['Hello', 'Welcome', ',', 'to', 'Vansh', 'NLP', 'Tutorials', '.']\n",
      "2. words are['Please', 'do', 'watch', 'the', 'entire', 'couse', '!']\n",
      "3. words are['to', 'become', 'expert', 'in', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "for idx, sent in enumerate(documents,start=1):\n",
    "    print(f\"{idx}. words are{word_tokenize(sent)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8df2e4e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Vansh',\n",
       " 'NLP',\n",
       " 'Tutorials.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'couse',\n",
       " '!',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To include . with the last word this TreeBankWordTokenizer library is used\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tok = TreebankWordTokenizer()\n",
    "tok.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7034e3",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6513b5",
   "metadata": {},
   "source": [
    "#### 1. PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a05ac359",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"eating\",\"eat\",\"eaten\",\"writing\",\"write\",\"wrote\",\"learn\",\"learning\",\"learned\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6bf6039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating...>eat\n",
      "eat...>eat\n",
      "eaten...>eaten\n",
      "writing...>write\n",
      "write...>write\n",
      "wrote...>wrote\n",
      "learn...>learn\n",
      "learning...>learn\n",
      "learned...>learn\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemming = PorterStemmer()\n",
    "for word in words:\n",
    "    print(word+ \"...>\"+ stemming.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42609655",
   "metadata": {},
   "source": [
    "#### 2. RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38a7bbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "date\n",
      "admir\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "reg_stem = RegexpStemmer('ing$|s$|e$|able$',min=4)\n",
    "print(reg_stem.stem(\"eating\"))\n",
    "print(reg_stem.stem(\"dates\"))\n",
    "print(reg_stem.stem(\"admirable\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ff18d1",
   "metadata": {},
   "source": [
    "#### 3. Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0acd3498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating--->eat\n",
      "eat--->eat\n",
      "eaten--->eaten\n",
      "writing--->write\n",
      "write--->write\n",
      "wrote--->wrote\n",
      "learn--->learn\n",
      "learning--->learn\n",
      "learned--->learn\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snb = SnowballStemmer(language='english')\n",
    "for word in words:\n",
    "    print(word +'--->'+ snb.stem(word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7819ac58",
   "metadata": {},
   "source": [
    "## LEMMATIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017b9c68",
   "metadata": {},
   "source": [
    "### 1. Wordnet Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdf13fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating--->eat\n",
      "eat--->eat\n",
      "eaten--->eat\n",
      "writing--->write\n",
      "write--->write\n",
      "wrote--->write\n",
      "learn--->learn\n",
      "learning--->learn\n",
      "learned--->learn\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lmt = WordNetLemmatizer()\n",
    "for word in words:\n",
    "    print(word+\"--->\"+lmt.lemmatize(word=word,pos='v'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8b02f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\AMAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c063b340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of English words that are not useful in paragraph\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4a9bf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"Natural Language Processing (NLP) is a field of\" \\\n",
    "\" artificial intelligence that focuses on enabling computers to understand,\" \\\n",
    "\" interpret, and generate human language. It combines linguistics, computer science,\" \\\n",
    "\" and machine learning to process large amounts of natural language data. Common\" \\\n",
    "\" applications of NLP include text classification, sentiment analysis, machine\" \\\n",
    "\" translation, chatbots, and speech recognition. By bridging the gap between human \" \\\n",
    "\"communication and computer understanding, NLP plays a crucial role in making technology more \" \\\n",
    "\"interactive and intelligent.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c34038e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "481ed8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93a1c17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply Stopword then apply stemming\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words(\"english\"))]\n",
    "    sentences[i] = ' '.join(words) # Converting all the words in sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bf1868f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natur languag process ( nlp ) field artifici intellig focus enabl comput understand , interpret , gener human languag .',\n",
       " 'it combin linguist , comput scienc , machin learn process larg amount natur languag data .',\n",
       " 'common applic nlp includ text classif , sentiment analysi , machin translat , chatbot , speech recognit .',\n",
       " 'by bridg gap human commun comput understand , nlp play crucial role make technolog interact intellig .']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ed03935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natur languag process ( nlp ) field artifici intellig focus enabl comput understand , interpret , gener human languag .',\n",
       " 'combin linguist , comput scienc , machin learn process larg amount natur languag data .',\n",
       " 'common applic nlp includ text classif , sentiment analysi , machin translat , chatbot , speech recognit .',\n",
       " 'bridg gap human commun comput understand , nlp play crucial role make technolog interact intellig .']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Apply Stopword then apply snowball \n",
    "from nltk.stem import SnowballStemmer\n",
    "snowballstemmer = SnowballStemmer('english')\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [snowballstemmer.stem(word) for word in words if word not in set(stopwords.words(\"english\"))]\n",
    "    sentences[i] = ' '.join(words) # Converting all the words in sentences\n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aff4cb8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natur languag process ( nlp ) field artifici intellig focus enabl comput understand , interpret , gener human languag .',\n",
       " 'combin linguist , comput scienc , machin learn process larg amount natur languag data .',\n",
       " 'common applic nlp includ text classif , sentiment analysi , machin translat , chatbot , speech recognit .',\n",
       " 'bridg gap human commun comput understand , nlp play crucial role make technolog interact intellig .']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Apply Stopword then apply lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordlem = WordNetLemmatizer()\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [wordlem.lemmatize(word,pos=\"n\") for word in words if word not in set(stopwords.words(\"english\"))]\n",
    "    sentences[i] = ' '.join(words) # Converting all the words in sentences\n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1596c9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bridg', 'gap', 'human', 'commun', 'comput', 'understand', ',', 'nlp', 'play', 'crucial', 'role', 'make', 'technolog', 'interact', 'intellig', '.']\n"
     ]
    }
   ],
   "source": [
    "words2 = nltk.word_tokenize(paragraph)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81c22cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\AMAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d259f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_elements = nltk.pos_tag(words2, tagset=None, lang='eng')\n",
    "nltk.ne_chunk(tag_elements).draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615d8fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tfenv)",
   "language": "python",
   "name": "tfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
